---
title: "Final Project: Identifying Qualitative Performance Category in Weight Lifting Exercise with Machine Learning Algorithm"
author: "Andrew Nix"
date: "4/20/2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

# Introduction
The goal of this project is to accurately predict the manner in which a weight lifting exercise was performed based on a wide range of accelerometer- measures gathered across six subject.  Details about the study and the data set can be found here: *http://groupware.les.inf.puc-rio.br/har*.  A random forrest model was generated based on a training data set consisting of 160 features (eventually narrowed down to 53 features after data cleaning was completed) and 19,622 observations across 6 subjects.  The model achieved an in-sample accuracy level of over 99%. The model's out-of-sample error was also estimated based on cross validation measures and the use of a segregated "validation" data set (20% of the original "training set"), and  found to meet the 99% accuracy threshold as well.  
The model was then applied to a "testing" data set where the actual performance categories were unknown. 

#Data Loading and Overview

The data and the necessary libraries were loaded and a review of the training set was conducted (in the interest of brevity details not included here). 

The code used to load the csv files is below.

```{r,eval=FALSE}
## Library Loading 
#### Data loading ####
Accelo_Training  <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv', 
                             na.strings=c('#DIV/0', '', 'NA') ,stringsAsFactors = F)

Accelo_Testing  <- read.csv('https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv',
                            na.strings= c('#DIV/0', '', 'NA'),stringsAsFactors = F)

```

#Data Cleaning
It was recognized that a large number of columns consisted almost entirely of NA values.  These columns were deemed to be unhelpful to the data analysis and were removed.  

```{r,eval=FALSE}
## check NA count by column

na_count <-sapply(Accelo_Training, function(y) sum(length(which(is.na(y)))))
# na_count

## 100 of the 160 columns are missing 19216 observations (out of hte total of 19622 observations).  I went ahead and removed these columns from the dataset with the following code

NA_ColNames <- names(sapply(na_count[na_count>10000], names))

Accelo_Training<- Accelo_Training[setdiff(names(Accelo_Training),NA_ColNames)]
```


In addition, seven other variables (X, user_name,raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window and num_window) were judged to be not appropriate for inclusion in my model for the following reasons:
1) they are not directly associated with the phenomenon of interest (e.g. the identify of the subject should not matter in the prediction); and  
2) their inclusion risked an increase in out-of-sample error (e.g. the identity of the subject may help predict outcomes in the sample but would not be helpful for predictions involving other subjects).

```{r,eval=FALSE}

Accelo_Training_Analysis <-Accelo_Training[,-(1:7)]
```

# Exploratory Analysis 

A correlation matrix was generated to examine the relationships among the variables.  The subset of correlations that are above 0.90 are shown below.

```{r,echo=FALSE,message=FALSE,warning=FALSE}
##### IDENTIFY ALL STRONGLY CORRELATED VARIABLES AND DETERMINE IF VARIABLES SHOULD BE REMOVED AND/OR COMBINED ######
library(dplyr)
library(reshape2)
load("Accelo_Training_Analysis")
Accelo_CorrMatrix <- as.matrix(cor(Accelo_Training_Analysis[,-53]))
Accelo_CorrMatrix_melt <- arrange(melt(Accelo_CorrMatrix), abs(value))
Accelo_HighlyCorr <- subset( Accelo_CorrMatrix_melt, abs(value) > .75 & abs(value)<1)
 subset( Accelo_HighlyCorr, value > .90)
 
```

Although a large number of highly correlated variables were identified, all variables were still retained at this stage.  This choice was made in order to maximize the predictive power in the event that a model was chosen that is largely unaffected by issues of multi-collinearity (such as random forrest-based model).

Relationships among individual features, as well as amongst multiple features (via principle component analysis, combining weighted combinations of individual predictors into components), failed to distinguish the "classe" groups.  See the below PCM plot for an illustration of this finding.  

```{r, echo=FALSE,message=FALSE,warning=FALSE}
#######   PCA ANALYSIS #######

### PCA without any data transformation ####
Accelo_PrincComp <- prcomp(Accelo_Training_Analysis[,-53],scale=TRUE)

## plot the top 2 principal components with colors added corresponding to the "classe" values 
plot(Accelo_PrincComp$x[,1],Accelo_PrincComp$x[,2],xlab="PC1",ylab="PC2",col=as.factor(Accelo_Training_Analysis[,53])) ## plots PC1 and PC2

```

The colors, representing the individual classe groups, almost entirely overlap each other in a plot of the top 2 principle components.  This suggests that these components, by themselves, only have limited predictive power for the classe variable.  

For these reasons, I decided to focus on an approach that emphasized predictive accuracy rather than on the influence of individual predictors.

# Model Selection 
A random forrest model was selected as the method for training the data set due to it's well-established reputation for accuracy in categorical prediction.  

The first iteration of a random forrest model using default paramaters was found to have both Accuracy and Kappa values above 0.99 and an optimal mtry value of 2 (based on the default setting for *ntree*, i.e. number of trees is 500).  

The final iteration (presented below) was essentially unchanged from this first version of th emodel.  The only real adjustment to the code was to incorporate parallel processing to improve model efficiency.  This proved to be a very effective approach, as the model run time (as evidence by the log data also provided by the code) improved by around 3000% with these changes. 

```{r,eval=FALSE,warning=FALSE,message=FALSE}

## Configure parallel processing and trainControl object

cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE)

## Training model incorporating trainControl settings for parallel processing 

Rprof ( templog <- tempfile(),  memory.profiling = TRUE )
seed<-2222 
set.seed(seed)
control <- trainControl(method="repeatedcv", number=10, repeats=3)
RF_Default_Fit <-train(x = Accelo_training[,-53], y = Accelo_training$classe,method="rf",
##,metric=metric, tuneGrid=tunegrid,
trControl=fitControl)

## De-register parallel processing cluster upon completion of the main query used to build the model 

stopCluster(cluster)
registerDoSEQ()
```

As expected, the overall model fit once again had accuracy and Kappa values well above 0.99 based on the training data. 

An interesting side note is that the model built using the samller "training" data set (80% of the data used in generating the "default" model), found an optimal mtry value  of 27.  This represents a large jump from the mtry value of 2 noted earlier.  However, considering that the optimal mtry value changed significantly between the 2 data sets and that the mtry value  appears to be fairly un-important overall (accuracy stays between 0.993 and 0.994 when the total number of predictors per tree is between 2 and 27 and the value stays above 0.99 for mtry values as high as 52), any mtry value between 2 and 27 is probably acceptable. 

# Evaluating Cross Validation and Out-of-Sample Error

The following code was run to evaluate model fit, including the assessment of both in-sample and estimated out-of-sample error.


```{r,cache=TRUE,warning=FALSE,message=FALSE}
### OVERALL MODEL FIT
load("RF_Default_Fit")
load("Accelo_valid")
library(caret)

RF_Default_Fit

### Cross Validated Samples
RF_Default_Fit$resample

### Confusion matrix with Cross-Validated Samples
confusionMatrix.train(RF_Default_Fit)

#### APPLY THE MODEL TO THE "VALIDATION" DATA SET 

pred_valid <- predict(RF_Default_Fit,Accelo_valid); 

## COMPARE ACCURACY BETWEEN TRAINING AND VALIDATION DATA SETS ###
confusionMatrix(pred_valid,Accelo_valid$classe)

```

The individual cross-validated samples each produced highly accurate results and near-identical predictions (per the cross-validated confusion matrix).

Further support for the accuracy of my model was established by applying the model from the "training" set to the "validation" set and determining the accuracy of those predictions based on the known outcome (see the confusion matrix for details).  The accuracy and Kappa values are, in fact, even higher in the validation data set than in the training set.  

All these results, when taken together, suggest that --under the assumption that the in-sample observations are representative of the general population -- the predictions from my model are highly reliable and should apply to out-of-sample data, as well as in-sample.

#Impact of the Number of Trees on Model
It should be noted that the the ntrees paramater (the number of trees to be included in the model) was also evaluated to determine whether the addition of a modest number of additional trees (550 or 600 compared to the default value of 500) would make a meaningful difference in the model.

```{r,cache=TRUE}
# compare results
load("modellist")
results <- resamples(modellist)
summary(results)
dotplot(results)

```

The results of this evaluation suggest that adding additional trees to the model would not be worth the additional "cost" in terms of processing requirements.  For example, adding 100 trees above the default value of 500 resulted in no improvement in the model whatsoever.

# Predicting the Performance Category in the Test Data
Finally, i was ready to make predictions using a new data sample consisting of 20 observations.  The results of these predictions are provided below.


```{r,cache=TRUE}
load("pred1")
pred1
```


#Concluding Comments Regarding Future Predictions Using this Model
Although the true values for these observations are unknown, I have a high confidence level -- based on the cross-validated model assessments, as well as the model's performance in the "validation" test set--, that these observations will be accurately predicted by my model.

That said, there does remain a caveat. 

My high confidence level is, in part, due to the fact that the  "test" set includes the same set of six subjects who were the basis for the observations in the training set.  While it may turn out to be the case that this limited subject sample size does not result in the overfitting of the model, this will remain a risk until validation can be performed based on observations taken from a new set of subjects (preferably randomized and representing a diverse population, based on age, gender, body type, etc.) 


